# 图神经网络相关文献综述

## 1 Semi-Supervised Classification With Graph Convolutional Networks

- 任务：对图上数据结构的**半监督**学习
  - 任务具体描述：对Graph **G = (V, E)**上的**N**个nodes $v_i \in V$进行分类，但是**仅有一小部分**nodes有labels(是标注数据)
  - 任务涉及的符号：$X_i$为**node i**的**feature vector**,$X = [X_1,...,X_n]$为图的结点特征矩阵，$A$为图的邻接矩阵，**$D = diag(D_{ii}), D_{ii} = \sum_{j}A_{ij}$**为图的度数矩阵

- 方法：使用神经网络模型$f(X,A)$对所有带标签结点进行supervised learning的训练
在图的邻接矩阵上调整$f(\cdot)$使得模型可以从supervised loss $L_0$分配梯度信息，学习所有nodes的表示
    - 神经网络结构
      <img src="./images/GCN1.png" width=500></img>
        - 输入：C个input channel,每个结点$v_i$特征向量$X_i \in R^C$
        - 中间：若干隐藏层
        - 输出：F个feature maps,每个结点$v_i$的输出为一个F维向量，$Y_i$为结点$v_i$的标签
    - 隐藏层的分层传播规则：$H^{(l + 1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$
      - 参数和传播
        - 传播范围：输入$\to$下一个隐藏层$\to$下下个隐藏层$\to$...$\to$输出层
        - $\tilde{A} = A + I_N$是无向图$G$加上自连接后的邻接矩阵
        - $\tilde{D_{ii}} = \sum_j \tilde{A_{ij}}$,$W^{(l)}$是layer-specific
        - $\sigma(\cdot)$是activation function,$H^{(l)}$是第$l$层activation function的输出矩阵，$H^{(0)} = X$
        - 这个传播规则可以通过**谱图卷积的局部一阶近似**来实现传播
      - **谱图卷积**
        - 第一种图卷积公式：$g_\theta \star x = U g_\theta U^Tx$
          - 参数：$x$为node的特征向量，$g_\theta = diag(\theta), \theta \in R^N$为卷积核,$\theta$为参数，$U$为图的laplace矩阵$L = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}} = U\Lambda U^T$的特征向量矩阵
          - 这种卷积的问题：复杂度过高，卷积核选取也不合适
        - **改进的卷积公式**：$g_{\theta'}\star x = \sum_{k = 0}^K\theta_k'T_k(\tilde{L})x$,$\tilde{L} = \frac{2}{\lambda_{max}}L - I_N$
          - 切比雪夫多项式
          $T_k(x) = 2xT_{k - 1}(x) - T_{k - 2}(x), T_0(x) = 1, T_1(x) = x$
          - 改进思路：$g_\theta(\Lambda)$可以用切比雪夫多项式$T_k(x)$的$K^{th} order$截断表达式来进行很好的估计$\to$$g_{\theta'}(\Lambda)\approx \sum_{k = 0}^K \theta_k'T_k(\tilde{\Lambda})$,$\tilde{\Lambda} = \frac{2}{\lambda_{max}}\Lambda - I_N$,$\lambda_{max}$是$L$最大的特征值,$\theta' \in R^k$是切比雪夫参数向量
          - 此公式为拉普拉斯算子中的$K^{th}$阶多项式，即它仅取决于**离中央结点最大K步**的结点
          - 通过这个改进的卷积公式，可以**堆叠建立多层卷积层(实现传播规则)**
      - 图谱卷积应用:**线性模型**和**特征映射公式**
        - 线性模型公式:考虑$K = 1, \lambda_{max} = 2$,模型就只有2个参数，即$g_{\theta'}\star x \approx \theta_0'x + \theta_1'(L - I_N)x = \theta_0'x - \theta_1'D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x$
          - 自由参数：$\theta_0',\theta_1'$
          - 连续应用这种形式的filter,就可以有效卷积结点的$k^{th}$邻域， 其中$k$是模型中连续filter操作或卷积层的数目
        - 特征映射公式：feature maps(特征映射)$Z = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}X\Theta$
          - 考虑具有C个input channel(每个结点C维特征向量)的信号$X \in R^{N \times C}$,和$F$个filters，$\Theta \in R^{C\times F}$是filter参数矩阵，$Z \in R^{N\times F}$为卷积信号矩阵，filtering operation复杂度为$O(|E|FC)$

- 优缺点
  - 优点
    - **免除了传统方法中"相连结点应该相似，具有相同标签"这个及其严格的假设**，去除了损失函数中的正则项
    - 简单，表现优越，能通过提出的图谱卷积的局部一阶近似的方法来一个卷积层仅处理一截一阶邻居特征，然后通过分层传播规则叠加多个卷积层来达到多阶邻居特征传播
  - 缺点
    - 半监督时**带label的结点过少**，GCN**性能下降**严重
    - **浅层**GCN网络**不能大范围传播**label信息
    - **深层**GCN网络可能会导致**过度平滑**的问题

## 2 How Powerful are Graph Neural Networks?

- 主要内容
  - 证明**Weisfeiler-Lehman(WL) test图同构测试是GNN表示能力的上限**
  - **给出了一套理论框架**，分析主流GNNs在捕获图结构上的问题
  - 根据理论框架提出**如何构建和WL一样有效的GNN**，并给出了**图同构网络(GIN)**的构建，证明其表达能力和WL测试一样

- 一些概念和定义
  - multiset:允许有重复元素的集合，**本文的multiset指的是结点邻居的特征向量的集合**
  - 符号定义
    - 图:$G = (V, E)$
    - $A$为$G$的邻接矩阵
    - $D = diag(d_1,...,d_n)$为$G$的度数矩阵，$d_i = \sum_jA_{ij}$
    - $y_i \in \{0, 1\}^C$表示$C$维的结点one-hot label
    - $\{G_1,...,G_N\} \sube G$表示一个图的集合
    - $X_v$表示结点$v$的特征向量
    - $h_v$表示需要学习的结点$v$的表示向量
    - $h_G$为图$G$的表示向量
    - $y_G = h(h_G)$表示整个图$G$预测的标签

- GNN概述
  - 目标：以**图结构数据和结点特征作为输入**，学习到**结点/图的表示**，用于分类任务(结点分类或者图分类)
  - 基于邻域聚合的GNN的三个模块
    - **Aggregate**:聚合一阶邻域特征
    - **Combine**:将邻居的特征和当前结点特征**合并**，以**更新**当前结点特征
    - **Readout**:如果对图进行分类，需要将图中所有结点特征转变为图特征
  - k层GNNs的结构表示
    - $a_v^{(k)} = \text{AGGREGATE}^{(k)}(\{h_{u}^{(k - 1)}, u \in N(v)\}), h_v^{(k)} = \text{COMBINE}(h_v^{(k - 1)}, a_v^{(k)})$,$N(v)$表示结点$v$的邻居的集合
  - 分类任务
    - 结点分类：结点在最后一层的表示$h_v^{(K)}$可以用于预测
    - 图分类：要将graph中所有结点特征变成graph特征$h_G = \text{READOUT}(\{h_v^{(K)}|v \in G\})$

- WL test图同构测试
  - 图同构：对图$G_1, G_2$，存在一一映射$f$,将$G_1$的结点集合$V_1$映射到$G_2$的结点集合$V_2$上，且映射后$G_1 = G_2$
  - WL test:通过**不断迭代得到结点新label**来判断同构图的同构性
    - **Aggregate**：聚合每个结点邻域和自身label
    - **Update Label**：使用**Hash映射**结点聚合label作为结点新label
  - WL test举例
  <img src="./images/WL test.png" width=700></img>
    - (a) 网络中每个结点有1个label
    - (b) label扩展:做一阶广度搜索，只遍历自己的邻居
    - (c) label压缩：将扩展的label映射为一个新label
    - (d) 压缩label替换扩展label
    - (e) 对label进行计数，label的个数作为网络的新特征

- WL test和GNN
  - **WL test是GNN表示能力的上限**
  引理：令两个图$G_1,G_2$是任意两个**非同构**的图，如果存在一个图神经网络$A: G\to R^d$将$G_1,G_2$映射到不同的embedding，那么通过WL图同构测试也可以确定$G_1,G_2$是非同构的
  - **和WL test能力相同的GNN**
  定理：令$A: G\to R^d$是一个GNN，对于两个通过WL图同构测试测定为**不同构**的两个图$G_1,G_2$,在GNN层足够多的情况下，如果下面情况成立，则通过GNN可以将这两个图映射到不同的embedding：
  (1) $A$用下面的公式迭代来聚合和更新结点特征：
  $h_v^{(k)}= \phi(h_v^{(k- 1)}, f(\{h_u^{(k - 1)}| u \in N(v)\}))$
    - 函数$f$作用在**multisets**上
    - $\phi$函数是单射的

- 图同构网络**GIN**
  - 定理：当$X$可数时，将**Aggregate**设置为**sum**,**Combine**设置为$1 + \epsilon$时，存在$f(x)$使得$h(c, X)$为单射：$h(c, X) = (1 + \epsilon) \cdot f(c) + \sum_{x \in X}f(x), c$为结点特征，$X$为邻域特征集合
  - 推论：$\forall g(c, X)$,可以分解为$f\circ \varphi$形式，满足单射性$\to g(c, X) = \varphi((1 + \epsilon)\cdot f(c) + \sum_{x \in X}f(x))$
  - GIN的结构框架
  通过引入多层感知机MLP来学习$\varphi,f$，保证单射性，最后得到**MLP+SUM**的GIN框架
  $h_v^{(k)} = \text{MLP}^{(k)}((1 + c^{(k)})\cdot h_v^{(k - 1)} + \sum_{u \in N(v)}h_u^{(k - 1)})$
    - MLP可以近似拟合任何函数，因此可以学到单射函数
    - 约束输入特征是one-hot，所以第一次迭代sum后还是满足单射性，不需要做MLP的预处理
    - 迭代一轮的新特征$h_v^{(k)}$可数，经过了$f(x)$转换，下一轮迭代还是满足单射性条件
  - GIN的图分类任务
    - 考虑readout函数，以所有结点的embedding为输入，输出整个图的embedding
    - **READOUT模块**使用**Concat+Sum**，对每次迭代得到的所有结点特征求和得到图的特征，然后拼接起来
    - $h_G = \text{CONCAT}(\text{sum}(\{h_v^{(k)}| v\in G\})| k=0,1,...,K)$

- 优点
  - 从理论上给出了GNN的上限
  - 建立了一套理论框架，能对GNN的表达能力进行分析
  - 设计了图同构网络GIN，达到了GNN表示能力上限



## 3 GRAPH ATTENTION NETWORKS（GAT）

- 主要内容

  - 提出一种GAT（Graph Attention Networks）网络
    - 使用masked self-attention层
      - 根据每个结点在其相邻节点上的attention来对结点表示进行更新
    - 可以根据邻居结点的特征为每个结点分配不同的权值
      - 无需事先知道图的结构
    - 解决了之前基于图卷积（或其近似）模型所存在的问题
    - 可以有效地适用于基于图的归纳学习问题与转导学习问题

- Model

  - Graph Attentional Layer

    - 首先说明其输入输出的feature

      - 输入是一个结点特征向量集$\textbf{h} = \{\overrightarrow{h_1}, ..., \overrightarrow{h_N}\}, \overrightarrow{h_i} \in \mathbb{R}^F$。其中N表示结点集中结点的个数，F表示特征向量的维度
      - 输出是一个新的节点特征向量集$\textbf{h}' = \{\overrightarrow{h'_1}, ..., \overrightarrow{h'_N}\}, \overrightarrow{h'_i} \in \mathbb{R}^{F'}$，其中F'表示新的结点特征向量维度（可以不等于F）

    - 其结构如图所示：

      <img src="./images/GAT PICTURE1.png" width=500></img>

    - 右图：结点$\overrightarrow{h}_1'$在邻域中具有多端连接，不同的箭头样式表示独立的注意力计算，通过concat（直连）或avg（平均）获取$\overrightarrow{h}_1'$

    - 首先根据输入的结点特征向量集进行self-attention处理：
      $$
      e_{ij} = a(\textbf{W} \overrightarrow{h_i}, \textbf{W} \overrightarrow{h_j})
      $$

    - 其中，a是一个 \mathbb{R}^{F'} \times \mathbb{R}^{F'} \rightarrow \mathbb{R}的映射（称作注意力机制），W \in \mathbb{R}^{F' \times F}是一个权值矩阵（被所有\overrightarrow{h_i}所共享）

    - 一般来说，self-attention会将注意力分配到图中所有节点上，这种做法显然会丢失结构信息

    - 为解决这一问题，本文使用一种masked attention的方式：仅将注意力分配到节点i的相邻结点集上，即令$ j \in \mathcal{N}_i $（本文中默认$i$属于$\mathcal{N}_i$）（见上面左图）:
      $$
      \alpha_{ij} = softmax_j(e_{ij}) = \frac{exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} exp(e_{ik})}
      $$
      
    - 本文中，计算attention coefficient的函数a使用单层的前馈神经网络实现，使用LeakyReLU作为激活函数，即
    
    $$
    e_{ij} = LeakyReLU(\overrightarrow{a}^T [\textbf{W} \overrightarrow{h}_i || \textbf{W} \overrightarrow{h}_j]), \\
            \alpha_{ij} = \frac{exp(LeakyReLU(\overrightarrow{a}^T [\textbf{W} \overrightarrow{h}_i || \textbf{W} \overrightarrow{h}_j]))}{\sum_{k \in \mathcal{N}_i} exp(LeakyReLU(\overrightarrow{a}^T [\textbf{W} \overrightarrow{h}_i || \textbf{W} \overrightarrow{h}_k]))}, \\
            （||表示连接） \\
            其中\overrightarrow{a}^T \in \mathbb{R}^{2F'}为前馈神经网络a的参数\\
    $$
    
    
    
      - 现在求出了$\alpha_{ij}$，于是就可以得到 $\overrightarrow{h'}_i$：
        $$
        \overrightarrow{h'}_i = \sigma(\sum_{j \in \mathcal{N}_i} \alpha_{ij} \textbf{W} \overrightarrow{h}_j)
        $$
        
    - multi-head attention
    
    - 为提高模型的拟合能力，引入了multi-head attention机制（见上面右图），即不只用一个函数a来计算attention coefficient，而是设置K个函数，每一个函数都能计算出一组attention coefficient，并能计算出一组加权求和用的系数$\alpha_{ij}$，每一个卷积层中，K个attention机制独立工作，分别计算出结果后连接在一起，得到卷积的结果，即
        $$
        \overrightarrow{h'}_i = \parallel^{K}_{k=1} \sigma(\sum_{j \in \mathcal{N}_i} \alpha_{ij}^k \textbf{W}^k \overrightarrow{h}_j)
        $$
        
    - 特别地，如果在最后一层（即prediction layer）中，我们仍然使用multi-head attention机制，那么久不采取连接（concat）的方式来合并不同注意力机制的结果，而是采用求平均（avg）的方式：
$$
\overrightarrow{h'}_i =  \sigma(\frac{1}{K} \sum^K_{k=1} \sum_{j \in \mathcal{N}_i} \alpha_{ij}^k \textbf{W}^k \overrightarrow{h}_j)
$$


- 模型比较与模型优点
  
  
  - 运行高效
    
    - 特别地，需进行特征值分解等复杂的矩阵运算，单层GAT的时间复杂度与GCN相同，为$O(|V|FF' + |E|F')$，其中$|V|$和$|E|$分别表示图中的结点数量和边的数量
    - 为每个结点和其每个相邻结点计算attention时可以进行并行计算
  
  - 相较于GCN，GAT可以按照规则指定neighbor不同的权重，不受邻居数目不同的影响，具有更强的表示能力
  - 对于图中的所有边，attention机制是共享的。因此GAT也是一种局部模型。也就是说，在使用GAT时，我们无需访问整个图，而只需要访问所关注节点的邻节点即可
    - 可以处理有向图（若$i \rightarrow j$ 不存在，仅需忽略 $\alpha_{ij}$ 即可)
    - 可以被直接用于解决归纳学习问题，即可以对从未见过的图结构进行处理
  - GraphSAGE方法通过从每个节点的邻居中抽取固定数量的节点，从而保证其计算的一致性。这意味着我们在对一个结点执行推断时，无法访问其所有的邻居；而GAT是建立在所有邻节点上的，而且无需假设任何节点顺序
  - GAT可以被看作是MoNet的一个特例。具体来说，可以通过将伪坐标函数（pseudo-coordinate function）设为 $O(|V|FF' + |E|F')$ ，其中， $f(x)$表示结点 $x$ 的特征，$||$表示连接符号；相应的权值函数则变成了 $w_j(u)=softmax(MLP(u))$ 



## 4 Graph Neural Networks for Social Recommendation

- 主要内容

  - 关于基于GNN建立社交推荐系统提出三个问题：
    - 如何整合user-user graph和user-item graph
    - 如何捕捉user和item之间的联系以及user对item的意见
  - 如何区别社交关系的重要性
  - 提供了一个原则性的方法来联合user-item graph中的交互和意见
  - 提出了GraphRec框架，对上述两个graph和异构强度进行了连贯的建模

- 符号定义

  - $h_i^I$表示item空间用户$i$的embedding表示
  - $x_{ia}$表示用户i和与其邻接的物品a的聚合向量
  - $C_i$表示用户i在item-space中与其邻接的所有item的集合
  - $h_i^S$表示social空间用户$i$的embedding表示
  - $h^I_o$是item-space的用户o的向量表示
  - $N_i$表示用户i在social-space中与其邻接的所有用户的集合
  - $\mu _{jt}$表示注意力系数
  - $h_i$是user $i$最终的向量表示
  - $z_j$是item $j$最终的向量表示

- 总体框架

  <img src="./images/GNN for social recommendation PICTURE 1.jpg"></img>

  整个框架包含三个部分：

  - User Modeling，其中又包含两个部分：
    - Item Aggregation：利用user与item之间的联系生成一个embedding
    - Social Aggregation：利用user与user之间的关系生成一个embedding
    - User Modeling部分的输出就是由上述两个embedding连接成的向量
  - Item Modeling，利用user与item之间的关系以及对item的意见产生一个embedding
  - Rating Prediction，该部分利用上述两部分得到的向量，对user对item的打分进行预估

- User Modeling

  - Item Aggregation
    $$
    h_i^I = \sigma(W·Aggre_{items}(\{x_{ia} | \forall a \in C_i \}) + b) \\
    其中x_{ia} = g_v([q_a \oplus e_r])\\
    其中q_a为item的embedding，e_r为opinion的embedding，g_v为MLP\\
    Aggre_{items}可以如下表示：\\
    h_i^I = \sigma(W· \{ \sum_{a \in C(i)} \alpha_{ia}x_{ia} \} + b）\\
    \alpha_{ia}采用注意力网络学习user对item不同的权重：\\
      \alpha_{ia}^* = w_2^T · \sigma(W_1 · [x_{ia} \oplus p_i] + b_1) + b_2,\\
      \alpha_{ia} = \frac{exp(\alpha ^*_{ia})}{\sum_{a \in C(i)}exp(\alpha ^*_{ia})}
    $$


- Social Aggregation
  $$
  类似Item Aggregation，
  h_i^S = \sigma(W· \{ \sum_{a \in N(i)} \beta_{io}h^I_o \} + b）\\
  \beta_{io}^* = w_2^T · \sigma(W_1 · [h^I_o \oplus p_i] + b_1) + b_2,\\
  \beta_{io} = \frac{exp(\beta ^*_{io})}{\sum_{o \in N(i)}exp(\beta ^*_{io})}
  $$


  - User Modeling的最后将两个图产生的 $h^I_i$ 和 $h^S_i$ 连接，并送入MLP
    $$
    c_1 = [h^I_i \oplus h^S_i] \\
    ...\\
    c_{l-1} = \sigma(W_{l-1} · c_{l-2} + b_{l-1}) \\
    h_i = \sigma(W_l · c_{l-1} + b_l)
    $$

- Item Modeling


  - 类似之前的Item Aggregation，

$$
  f_{jt} = g_u([p_t \oplus e_r])\\
  z_j = \sigma(W· \{ \sum_{t \in B(j)} \mu_{jt}f_{jt} \} + b）\\
  \mu_{jt}^* = w_2^T · \sigma(W_1 · [f_{jt} \oplus q_j] + b_1) + b_2,\\
  \mu_{ia} = \frac{exp(\mu ^*_{jt})}{\sum_{t \in B(j)}exp(\mu ^*_{jt})}
$$

- Rating Prediction


  - 将User Modeling的输出$h_i$和Item Modeling的输出$z_j$结合，然后通过MLP进行非线性变换，得到最终的预测结果

$$
  g_1 = [h_i \oplus z_j] \\
  ...\\
  g_l = \sigma(W_l · g_{l-1} + b_l) \\
  r^{'}_{ij} = w^T · g_{l-1}
$$

- Model Training


  - 损失函数使用均方误差
  - 用户向量$h_i$、物品向量$z_j$、评分向量$e_r$，以及MLP中的$W$和$b$共同构成训练参数，三元组$(i, j, r)$表示用户$i$对物品$j$给出评分$r$
  - Optimizer：RMSprop
  - 使用Dropout解决Overfitting问题

- Experiment

  - 与一些baseline比较所得结果：

    <img src="./images/GNN for Social Recommendation PICTURE 2.png"></img>

  - 后续分别对社交网络和用户意见的影响、注意力机制的影响做出了分析，并对embedding的大小做了比较

- 后续工作

  - 探索user与item之间更丰富、复杂的属性
  - 考虑评分与社交关系的动态性



## 5 Session-based Recommendation with Graph Neural Networks

- 主要内容

  - 提出问题：以往的方法，如NARM（global and local RNN recommender），STAMP（captures users’ general interests and current interests, by employing simple MLP networks and an attentive net）等，以序列建模session来评估用户、items表示，但不能准确高效地获取sessions里的用户向量，忽略了items之间转换时的上下文
  - 提出将sessions建模成图结构数据，以使用GNN来捕捉items的嵌入向量
  - 不依赖于用户的相关表征，使用session的嵌入层来进行推荐

- 主要流程

  <img src="./images/Session-based Recommendation with Graph Neural Networks PICTURE 1.png"></img>

- 具体描述

  - $V = {v_1, ..., V_m}$表示所有sessions里包含的所有unique items；一个session按时间戳排序，表示为$ s = [ v_{s,1}, ... , v_{s,n}] $；每一个session s 建模为有向图$\mathcal{G} = (\mathcal{V}_s, \mathcal{E}_s)$，在session graph中，每一结点表示一个item $v_{s, i} \in V$，每条边 $(v_{s, i-1}, v_{s, i})$ 意味着用户在session s 内在点击 $ v{s, i-1} $ 后点击了 $ v_{s, i} $

  - 利用GNN进行Item Embedding：
    $$
    a^t_{s,i} = A_{s,i:}[\textbf{v}_1^{t-1}, ..., \textbf{v}_n^{t-1}] \textbf{H} + \textbf{b}, \\
    \textbf{z}^t_{s,i} = \sigma(\textbf{W}_z \textbf{a}^t_{s,i} + \textbf{U}_z \textbf{v}_i^{t-1}), \\
    \textbf{r}^t_{s,i} = \sigma(\textbf{W}_r \textbf{a}^t_{s,i} + \textbf{U}_r \textbf{v}_i^{t-1}), \\
    \widetilde{\textbf{v}}_i^t = tanh(\textbf{W}_o \textbf{a}_{s, i} + \textbf{U}_o(r^t_{s,i} \odot v_i^{t-1}), \\
    \textbf{v}_i^t = (1 - \textbf{z}_{s,i}^t) \odot \textbf{v}_i^{t-1} + \textbf{z}_{s,i}^t \odot \widetilde{\textbf{v}}_i^t
    $$
    

    - 其中 $H \in \mathbb{R}^{d*2d}$ 控制权重，$z_{s, i}$和$r_{s, i}$分别是重置门和更新门，[\textbf{v}_1^{t-1}, ..., \textbf{v}_n^{t-1}]是session s的节点向量列表，\textbf{v}_i是节点v_{s, i}的潜在向量表示，连接矩阵A_s被定义为两个邻接矩阵A_s^{(out)}和A_s^{(in)}的拼接，决定graph中结点间如何彼此交互，A_{s, i}是节点v_{s,i}在A_s中对应的两列

  - 计算全局的session偏好

    - 对于局部的embedding，$s_l$定义为最后一个点击的item$v_n$，采取软注意力机制得到global preference：
      $$
      \alpha_i = \textbf{q}^T \sigma (W_1v_n + W_2v_i + c), \\
          \textbf{s}_g = \sum^n_{i=1} \alpha_i \textbf{v}_i
      $$
      
    - 参数q和W_1、W_2控制item embeddings向量的权重
    
      - 混合embedding表示：
        $$
        \textbf{s}_h = \textbf{W}_3[\textbf{s}_l; \textbf{s}_g]
        $$
      
    - 矩阵W_3将两个合并的embedding向量压缩到潜在空间R^d
    
  - Prediction and Training：
  
    - 计算每一候选item $v_i \in V$ 的得分，并归一化：
  - 矩阵W_3将两个合并的embedding向量压缩到潜在空间R^d
  
  - Prediction and Training：
  
    - 计算每一候选item $v_i \in V$ 的得分，并归一化：
      $$
    \hat{\textbf{z}_i} = \textbf{s}_h^T \textbf{v}_i \\
        \hat{y} = softmax(\hat{\textbf{z}})
      $$
  
    - 使用交叉熵定义损失函数：
      $$
      \mathcal{L}(\hat{\textbf{y}}) = - \sum^m_{i=1} \textbf{y}_ilog(\hat{\textbf{y}_i}) + (1 - \textbf{y}_i)log(1 - \hat{\textbf{y}}_i)
      \$$
      $$
  
    - 模型优化使用BPTT(Back-Propagation Through Time)进行优化训练
  
- Experiment

  - 与一些baseline的比较

    <img src="./images/Session-based Recommendation with Graph Neural Networks PICTURE 2.png"></img>

  - 不同连接模式下的模型效果

    <img src="./images/Session-based Recommendation with Graph Neural Networks PICTURE 3.png"></img>

    <img src="./images/Session-based Recommendation with Graph Neural Networks PICTURE 4.png"></img>
    
    

- 图中符号解释

  - SR-GNN-FC：session内items间全连接
  - SR-GNN-NGC：归一化全局连接
  - SR-GNN-L：只使用local embedding
  - SR-GNN-AVG：使用平均池化的global embedding
  - SR-GNN-ATT：采取注意力机制的global embedding