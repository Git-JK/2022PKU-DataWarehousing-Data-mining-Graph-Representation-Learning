# 图神经网络相关文献综述

## 1 Semi-Supervised Classification With Graph Convolutional Networks

- 任务：对图上数据结构的**半监督**学习
  - 任务具体描述：对Graph **G = (V, E)**上的**N**个nodes $v_i \in V$进行分类，但是**仅有一小部分**nodes有labels(是标注数据)
  - 任务涉及的符号：$X_i$为**node i**的**feature vector**,$X = [X_1,...,X_n]$为图的结点特征矩阵，$A$为图的邻接矩阵，**$D = diag(D_{ii}), D_{ii} = \sum_{j}A_{ij}$**为图的度数矩阵

- 方法：使用神经网络模型$f(X,A)$对所有带标签结点进行supervised learning的训练
在图的邻接矩阵上调整$f(\cdot)$使得模型可以从supervised loss $L_0$分配梯度信息，学习所有nodes的表示
    - 神经网络结构
    <img src="GCN1.png" width=500></img>
        - 输入：C个input channel,每个结点$v_i$特征向量$X_i \in R^C$
        - 中间：若干隐藏层
        - 输出：F个feature maps,每个结点$v_i$的输出为一个F维向量，$Y_i$为结点$v_i$的标签
    - 隐藏层的分层传播规则：$H^{(l + 1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$
      - 参数和传播
        - 传播范围：输入$\to$下一个隐藏层$\to$下下个隐藏层$\to$...$\to$输出层
        - $\tilde{A} = A + I_N$是无向图$G$加上自连接后的邻接矩阵
        - $\tilde{D_{ii}} = \sum_j \tilde{A_{ij}}$,$W^{(l)}$是layer-specific
        - $\sigma(\cdot)$是activation function,$H^{(l)}$是第$l$层activation function的输出矩阵，$H^{(0)} = X$
        - 这个传播规则可以通过**谱图卷积的局部一阶近似**来实现传播
      - **谱图卷积**
        - 第一种图卷积公式：$g_\theta \star x = U g_\theta U^Tx$
          - 参数：$x$为node的特征向量，$g_\theta = diag(\theta), \theta \in R^N$为卷积核,$\theta$为参数，$U$为图的laplace矩阵$L = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}} = U\Lambda U^T$的特征向量矩阵
          - 这种卷积的问题：复杂度过高，卷积核选取也不合适
        - **改进的卷积公式**：$g_{\theta'}\star x = \sum_{k = 0}^K\theta_k'T_k(\tilde{L})x$,$\tilde{L} = \frac{2}{\lambda_{max}}L - I_N$
          - 切比雪夫多项式
          $T_k(x) = 2xT_{k - 1}(x) - T_{k - 2}(x), T_0(x) = 1, T_1(x) = x$
          - 改进思路：$g_\theta(\Lambda)$可以用切比雪夫多项式$T_k(x)$的$K^{th} order$截断表达式来进行很好的估计$\to$$g_{\theta'}(\Lambda)\approx \sum_{k = 0}^K \theta_k'T_k(\tilde{\Lambda})$,$\tilde{\Lambda} = \frac{2}{\lambda_{max}}\Lambda - I_N$,$\lambda_{max}$是$L$最大的特征值,$\theta' \in R^k$是切比雪夫参数向量
          - 此公式为拉普拉斯算子中的$K^{th}$阶多项式，即它仅取决于**离中央结点最大K步**的结点
          - 通过这个改进的卷积公式，可以**堆叠建立多层卷积层(实现传播规则)**
      - 图谱卷积应用:**线性模型**和**特征映射公式**
        - 线性模型公式:考虑$K = 1, \lambda_{max} = 2$,模型就只有2个参数，即$g_{\theta'}\star x \approx \theta_0'x + \theta_1'(L - I_N)x = \theta_0'x - \theta_1'D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x$
          - 自由参数：$\theta_0',\theta_1'$
          - 连续应用这种形式的filter,就可以有效卷积结点的$k^{th}$邻域， 其中$k$是模型中连续filter操作或卷积层的数目
        - 特征映射公式：feature maps(特征映射)$Z = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}X\Theta$
          - 考虑具有C个input channel(每个结点C维特征向量)的信号$X \in R^{N \times C}$,和$F$个filters，$\Theta \in R^{C\times F}$是filter参数矩阵，$Z \in R^{N\times F}$为卷积信号矩阵，filtering operation复杂度为$O(|E|FC)$

- 优缺点
  - 优点
    - **免除了传统方法中"相连结点应该相似，具有相同标签"这个及其严格的假设**，去除了损失函数中的正则项
    - 简单，表现优越，能通过提出的图谱卷积的局部一阶近似的方法来一个卷积层仅处理一截一阶邻居特征，然后通过分层传播规则叠加多个卷积层来达到多阶邻居特征传播
  - 缺点
    - 半监督时**带label的结点过少**，GCN**性能下降**严重
    - **浅层**GCN网络**不能大范围传播**label信息
    - **深层**GCN网络可能会导致**过度平滑**的问题

## 2 How Powerful are Graph Neural Networks?

- 主要内容
  - 证明**Weisfeiler-Lehman(WL) test图同构测试是GNN表示能力的上限**
  - **给出了一套理论框架**，分析主流GNNs在捕获图结构上的问题
  - 根据理论框架提出**如何构建和WL一样有效的GNN**，并给出了**图同构网络(GIN)**的构建，证明其表达能力和WL测试一样

- 一些概念和定义
  - multiset:允许有重复元素的集合，**本文的multiset指的是结点邻居的特征向量的集合**
  - 符号定义
    - 图:$G = (V, E)$
    - $A$为$G$的邻接矩阵
    - $D = diag(d_1,...,d_n)$为$G$的度数矩阵，$d_i = \sum_jA_{ij}$
    - $y_i \in \{0, 1\}^C$表示$C$维的结点one-hot label
    - $\{G_1,...,G_N\} \sube G$表示一个图的集合
    - $X_v$表示结点$v$的特征向量
    - $h_v$表示需要学习的结点$v$的表示向量
    - $h_G$为图$G$的表示向量
    - $y_G = h(h_G)$表示整个图$G$预测的标签

- GNN概述
  - 目标：以**图结构数据和结点特征作为输入**，学习到**结点/图的表示**，用于分类任务(结点分类或者图分类)
  - 基于邻域聚合的GNN的三个模块
    - **Aggregate**:聚合一阶邻域特征
    - **Combine**:将邻居的特征和当前结点特征**合并**，以**更新**当前结点特征
    - **Readout**:如果对图进行分类，需要将图中所有结点特征转变为图特征
  - k层GNNs的结构表示
    - $a_v^{(k)} = \text{AGGREGATE}^{(k)}(\{h_{u}^{(k - 1)}, u \in N(v)\}), h_v^{(k)} = \text{COMBINE}(h_v^{(k - 1)}, a_v^{(k)})$,$N(v)$表示结点$v$的邻居的集合
  - 分类任务
    - 结点分类：结点在最后一层的表示$h_v^{(K)}$可以用于预测
    - 图分类：要将graph中所有结点特征变成graph特征$h_G = \text{READOUT}(\{h_v^{(K)}|v \in G\})$

- WL test图同构测试
  - 图同构：对图$G_1, G_2$，存在一一映射$f$,将$G_1$的结点集合$V_1$映射到$G_2$的结点集合$V_2$上，且映射后$G_1 = G_2$
  - WL test:通过**不断迭代得到结点新label**来判断同构图的同构性
    - **Aggregate**：聚合每个结点邻域和自身label
    - **Update Label**：使用**Hash映射**结点聚合label作为结点新label
  - WL test举例
  <img src="./images/WL test.png" width=700></img>
    - (a) 网络中每个结点有1个label
    - (b) label扩展:做一阶广度搜索，只遍历自己的邻居
    - (c) label压缩：将扩展的label映射为一个新label
    - (d) 压缩label替换扩展label
    - (e) 对label进行计数，label的个数作为网络的新特征

- WL test和GNN
  - **WL test是GNN表示能力的上限**
  引理：令两个图$G_1,G_2$是任意两个**非同构**的图，如果存在一个图神经网络$A: G\to R^d$将$G_1,G_2$映射到不同的embedding，那么通过WL图同构测试也可以确定$G_1,G_2$是非同构的
  - **和WL test能力相同的GNN**
  定理：令$A: G\to R^d$是一个GNN，对于两个通过WL图同构测试测定为**不同构**的两个图$G_1,G_2$,在GNN层足够多的情况下，如果下面情况成立，则通过GNN可以将这两个图映射到不同的embedding：
  (1) $A$用下面的公式迭代来聚合和更新结点特征：
  $h_v^{(k)}= \phi(h_v^{(k- 1)}, f(\{h_u^{(k - 1)}| u \in N(v)\}))$
    - 函数$f$作用在**multisets**上
    - $\phi$函数是单射的

- 图同构网络**GIN**
  - 定理：当$X$可数时，将**Aggregate**设置为**sum**,**Combine**设置为$1 + \epsilon$时，存在$f(x)$使得$h(c, X)$为单射：$h(c, X) = (1 + \epsilon) \cdot f(c) + \sum_{x \in X}f(x), c$为结点特征，$X$为邻域特征集合
  - 推论：$\forall g(c, X)$,可以分解为$f\circ \varphi$形式，满足单射性$\to g(c, X) = \varphi((1 + \epsilon)\cdot f(c) + \sum_{x \in X}f(x))$
  - GIN的结构框架
  通过引入多层感知机MLP来学习$\varphi,f$，保证单射性，最后得到**MLP+SUM**的GIN框架
  $h_v^{(k)} = \text{MLP}^{(k)}((1 + c^{(k)})\cdot h_v^{(k - 1)} + \sum_{u \in N(v)}h_u^{(k - 1)})$
    - MLP可以近似拟合任何函数，因此可以学到单射函数
    - 约束输入特征是one-hot，所以第一次迭代sum后还是满足单射性，不需要做MLP的预处理
    - 迭代一轮的新特征$h_v^{(k)}$可数，经过了$f(x)$转换，下一轮迭代还是满足单射性条件
  - GIN的图分类任务
    - 考虑readout函数，以所有结点的embedding为输入，输出整个图的embedding
    - **READOUT模块**使用**Concat+Sum**，对每次迭代得到的所有结点特征求和得到图的特征，然后拼接起来
    - $h_G = \text{CONCAT}(\text{sum}(\{h_v^{(k)}| v\in G\})| k=0,1,...,K)$

- 优点
  - 从理论上给出了GNN的上限
  - 建立了一套理论框架，能对GNN的表达能力进行分析
  - 设计了图同构网络GIN，达到了GNN表示能力上限