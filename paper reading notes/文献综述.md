# 图神经网络相关文献综述

## 1 Semi-Supervised Classification With Graph Convolutional Networks

- 任务：对图上数据结构的**半监督**学习
  - 任务具体描述：对Graph **G = (V, E)**上的**N**个nodes $v_i \in V$进行分类，但是**仅有一小部分**nodes有labels(是标注数据)
  - 任务涉及的符号：$X_i$为**node i**的**feature vector**,$X = [X_1,...,X_n]$为图的结点特征矩阵，$A$为图的邻接矩阵，**$D = diag(D_{ii}), D_{ii} = \sum_{j}A_{ij}$**为图的度数矩阵

- 方法：使用神经网络模型$f(X,A)$对所有带标签结点进行supervised learning的训练
在图的邻接矩阵上调整$f(\cdot)$使得模型可以从supervised loss $L_0$分配梯度信息，学习所有nodes的表示
    - 神经网络结构
    <img src="GCN1.png" width=500></img>
        - 输入：C个input channel,每个结点$v_i$特征向量$X_i \in R^C$
        - 中间：若干隐藏层
        - 输出：F个feature maps,每个结点$v_i$的输出为一个F维向量，$Y_i$为结点$v_i$的标签
    - 隐藏层的分层传播规则：$H^{(l + 1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$
      - 参数和传播
        - 传播范围：输入$\to$下一个隐藏层$\to$下下个隐藏层$\to$...$\to$输出层
        - $\tilde{A} = A + I_N$是无向图$G$加上自连接后的邻接矩阵
        - $\tilde{D_{ii}} = \sum_j \tilde{A_{ij}}$,$W^{(l)}$是layer-specific
        - $\sigma(\cdot)$是activation function,$H^{(l)}$是第$l$层activation function的输出矩阵，$H^{(0)} = X$
        - 这个传播规则可以通过**谱图卷积的局部一阶近似**来实现传播
      - **谱图卷积**
        - 第一种图卷积公式：$g_\theta \star x = U g_\theta U^Tx$
          - 参数：$x$为node的特征向量，$g_\theta = diag(\theta), \theta \in R^N$为卷积核,$\theta$为参数，$U$为图的laplace矩阵$L = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}} = U\Lambda U^T$的特征向量矩阵
          - 这种卷积的问题：复杂度过高，卷积核选取也不合适
        - **改进的卷积公式**：$g_{\theta'}\star x = \sum_{k = 0}^K\theta_k'T_k(\tilde{L})x$,$\tilde{L} = \frac{2}{\lambda_{max}}L - I_N$
          - 切比雪夫多项式
          $T_k(x) = 2xT_{k - 1}(x) - T_{k - 2}(x), T_0(x) = 1, T_1(x) = x$
          - 改进思路：$g_\theta(\Lambda)$可以用切比雪夫多项式$T_k(x)$的$K^{th} order$截断表达式来进行很好的估计$\to$$g_{\theta'}(\Lambda)\approx \sum_{k = 0}^K \theta_k'T_k(\tilde{\Lambda})$,$\tilde{\Lambda} = \frac{2}{\lambda_{max}}\Lambda - I_N$,$\lambda_{max}$是$L$最大的特征值,$\theta' \in R^k$是切比雪夫参数向量
          - 此公式为拉普拉斯算子中的$K^{th}$阶多项式，即它仅取决于**离中央结点最大K步**的结点
          - 通过这个改进的卷积公式，可以**堆叠建立多层卷积层(实现传播规则)**
      - 图谱卷积应用:**线性模型**和**特征映射公式**
        - 线性模型公式:考虑$K = 1, \lambda_{max} = 2$,模型就只有2个参数，即$g_{\theta'}\star x \approx \theta_0'x + \theta_1'(L - I_N)x = \theta_0'x - \theta_1'D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x$
          - 自由参数：$\theta_0',\theta_1'$
          - 连续应用这种形式的filter,就可以有效卷积结点的$k^{th}$邻域， 其中$k$是模型中连续filter操作或卷积层的数目
        - 特征映射公式：feature maps(特征映射)$Z = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}X\Theta$
          - 考虑具有C个input channel(每个结点C维特征向量)的信号$X \in R^{N \times C}$,和$F$个filters，$\Theta \in R^{C\times F}$是filter参数矩阵，$Z \in R^{N\times F}$为卷积信号矩阵，filtering operation复杂度为$O(|E|FC)$

- 优缺点
  - 优点
    - **免除了传统方法中"相连结点应该相似，具有相同标签"这个及其严格的假设**，去除了损失函数中的正则项
    - 简单，表现优越，能通过提出的图谱卷积的局部一阶近似的方法来一个卷积层仅处理一截一阶邻居特征，然后通过分层传播规则叠加多个卷积层来达到多阶邻居特征传播
  - 缺点
    - 半监督时**带label的结点过少**，GCN**性能下降**严重
    - **浅层**GCN网络**不能大范围传播**label信息
    - **深层**GCN网络可能会导致**过度平滑**的问题